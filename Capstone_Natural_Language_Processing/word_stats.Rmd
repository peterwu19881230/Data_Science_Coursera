---
title: "Word Stats"
author: "Peter Wu"
date: "1/10/2019"
output: html_document
---

#**Summary stats for 3 files**

####<br/> I intend to build a prediction algorithm by training the 3 available files: en_US.twitter.txt, en_US.news.txt, en_US.blogs.txt using Ngram (up to trigram). The Ngram algorithm takes every 1,2,3 words from the corpus of text to form a dictionary and will be able to predict the next occuring word upon an user's entry.

#####<br/> For simplicity of the report some code is not shown. Please refer to it here: https://github.com/peterwu19881230/datasciencecoursera/blob/master/Capstone/word_stats.Rmd


#<br/>1. Load the required libraries
```{r, message=F}
library(stringr) ##load to use the str_extract()
library(tidyverse)
library(wordcloud)
library(ngram)
```


#<br/>2. Read and clean the files
```{r,tidy=T,cache=T, message=F}
#function to read large txt file line by line & store as a vector
parseTXT=function(file){
  con = file(file, "r")
  
  dat=c()
  i=1
  while ( TRUE ) {
    line = readLines(con, n = 1) ##readLines will put escape characters if the line being read contains any meta characters
    
    ##stop parsing at the end of the file
    if ( length(line) == 0 ) {
      break
    }
    
    print(line) ##print the line being processed
    
    dat[i]=line
    i=i+1
    
  }
  close(con)
  
  return(dat)
}


##parse 3 files (too slow in Rmarkdown. I will just process them in R and load them)
###en_US.twitter=parseTXT("en_US.twitter.txt")
###en_US.news = parseTXT("en_US.news.txt")
###en_US.blogs = parseTXT("en_US.blogs.txt")

###txt_list=list(en_US.twitter=en_US.twitter,
###              en_US.news=en_US.news,
###              en_US.blogs=en_US.blogs)

###save(txt_list,file="txObjs.RData")
load("txtObjs.RData")

##Examine the structure
#str(txt_list)


```

#<br/>3. Build ngram models
```{r,tidy=T, message=F}
##Concatnate txt_list into 1 obj
#corpus=c(txt_list[[1]],txt_list[[2]],txt_list[[3]])
###save(corpus,file="corpus.RData")
###load("corpus.RData")

##function to remove , ? ! . ... from each word
###remove_marks=function(characters){
###  str_replace_all(characters,"[()?!,.;:\"&]","")
###}

##extract words and remove symbols from each corpus
###words= corpus %>% tolower %>% remove_marks
###save(words,file="words.RData")
###load("words.RData")

##function to filter sentences that have equal or less than n words
filter_sentence_and_sort=function(sentence_vector,n=1){
  if(n==1){
    indices=grep("\\S+",sentence_vector)
  }else if(n==2){
    indices=grep("\\S+\\s+\\S+",sentence_vector)
  }else if(n==3){
    indices=grep("\\S+\\s+\\S+\\s+\\S+",sentence_vector)
  }else{print("With this n the process is not written")
    return(0)
    }

  return(sentence_vector[indices][order(nchar(sentence_vector[indices]))])
}



##Due to performance issue, I will just use ~10% of the data to illustrate 
###start.time = Sys.time()
###one_gram=ngram(filter_sentence_and_sort(words,n=1)[1:430000],n=1)
###two_gram=ngram(filter_sentence_and_sort(words,n=2)[1:430000],n=2)
###three_gram=ngram(filter_sentence_and_sort(words,n=3)[1:430000],n=3)
###end.time = Sys.time()
###end.time-start.time #Time difference of 1.81902 mins


###save(one_gram,two_gram,three_gram,file="ngrams.RData")
###load("ngrams.RData")

###one_gram_tab=get.phrasetable(one_gram)
###two_gram_tab=get.phrasetable(two_gram)
###three_gram_tab=get.phrasetable(three_gram)

###save(one_gram_tab,two_gram_tab,three_gram_tab,file="ngram_tabs.RData")
load("ngram_tabs.RData")

```



#<br/>4.: Histogram to see how large and resourceful the 3 traning files are
```{r,tidy=T,cache=T,echo=FALSE, message=F,fig.width=13, fig.height=7}
##Function to do word count
count_word=function(txt) { sum(stringr::str_count(txt,"[a-zA-Z]+")) }

line.count=sapply(txt_list,length)
word.count=sapply(txt_list,count_word)

##Line count & word count
par(mfrow=c(1,2))

barplot(line.count,main="Line count")
y = format(1:8*5e6, scientific = FALSE)
barplot(word.count,yaxt="n",main="Word count")
axis(side=2,at=1:8*5e6,labels=y)
```



# <br/>5.: Histogram + wordcloud to instinctively visualize the structure of the corpus - using unigram, bigram and trigram
```{r,tidy=T,echo=FALSE, message=F,fig.width=14, fig.height=7}
#barplot + wordcloud showing frequency of word usage
par(mfrow=c(1,2))
barplot(one_gram_tab[[2]][1:10],names.arg=one_gram_tab[[1]][1:10],main="one_gram - Frequency of words")
set.seed(1)
wordcloud(one_gram_tab[[1]][1:100],freq=one_gram_tab[[2]][1:100],colors=brewer.pal(10, "Paired"))

par(mfrow=c(1,2))
barplot(two_gram_tab[[2]][1:10],names.arg=two_gram_tab[[1]][1:10],main="two_gram - Frequency of words")
set.seed(1)
wordcloud(two_gram_tab[[1]][1:100],freq=two_gram_tab[[2]][1:100],colors=brewer.pal(10, "Paired"))

par(mfrow=c(1,2))
barplot(three_gram_tab[[2]][1:10],names.arg=three_gram_tab[[1]][1:10],main="three_gram - Frequency of words")
set.seed(1)
wordcloud(three_gram_tab[[1]][1:100],freq=three_gram_tab[[2]][1:100],colors=brewer.pal(10, "Paired"))
```

#Proposal
#### I propose to use the above model built to predict the next word upon entry, based on frequencies or probability of the next occuring words. More sentences out of the corpus might be applied if the efficiency to build ngram models can be futher optimized.The resulting shiny app will include a text box that allows user to enter their favorite text and show what's the next word. If there is no prediction, the ouput will show "no predicted result available" 
```{r,tidy=T,echo=FALSE, message=F,fig.width=14, fig.height=7}
#This is just to make the above sentences properly wrapped
```
